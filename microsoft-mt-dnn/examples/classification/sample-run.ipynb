{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MT-DNN is an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid\n",
    "customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multitask knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The pip installable package and pretrained models will be publicly available at https://github.com/microsoft/mt-dnn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MT-DNN is designed for modularity, flexibility, and ease of use. These modules are built upon PyTorch (Paszke et al., 2019) and Transformers (Wolf\n",
    "et al., 2019), allowing the use of the SOTA pretrained models, e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c) and UniLM (Dong\n",
    "et al., 2019). The unique attribute of this package is a flexible interface for adversarial multi-task fine-tuning and knowledge distillation, so that researchers and developers can build large SOTA NLU models and then compress them to small ones\n",
    "for online deployment.The overall workflow and system architecture are shown in figures 1 and 3 respectively.\n",
    "\n",
    "\n",
    "![Workflow Design](https://nlpbp.blob.core.windows.net/images/mt-dnn2.JPG)\n",
    "\n",
    "The above figure shows workflow of MT-DNN: train a neural language model on a large amount of unlabeled raw text\n",
    "to obtain general contextual representations; then finetune the learned contextual representation on downstream tasks, e.g. GLUE (Wang et al., 2018); lastly, distill this large model to a lighter one for online deployment. In the later two phrases, we can leverage powerful multi-task learning and adversarial training to further improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overall_arch](https://nlpbp.blob.core.windows.net/images/mt-dnn.png)\n",
    "The figure above shows the overall system architecture. The lower layers are shared across all tasks while the top layers are taskspecific. The input X (either a sentence or a set of sentences) is first represented as a sequence of embedding\n",
    "vectors, one for each word, in l1. Then the encoder, e.g a Transformer or recurrent neural network (LSTM) model,\n",
    "captures the contextual information for each word and generates the shared contextual embedding vectors in l2.\n",
    "Finally, for each task, additional task-specific layers generate task-specific representations, followed by operations\n",
    "necessary for classification, similarity scoring, or relevance ranking. In case of adversarial training, we perturb\n",
    "embeddings from the lexicon encoder and then add an extra loss term during the training. Note that for the\n",
    "inference phrase, it does not require perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate MT-DNN models on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.  \n",
    "\n",
    "### Running Time\n",
    "\n",
    "This is a __computationally intensive__ notebook that runs on the entire MNLI dataset for match and mismatched datasets for training, development and test.  \n",
    "\n",
    "The table below provides some reference running time on a GPU machine.  \n",
    "\n",
    "|Dataset|MULTI_GPU_ON|Machine Configurations|Running time|\n",
    "|:------|:---------|:----------------------|:------------|\n",
    "|MultiNLI|True|4 NVIDIA Tesla K80 GPUs, 24GB GPU memory| ~ 20 hours |\n",
    "\n",
    "If you run into `CUDA out-of-memory error` or the jupyter kernel dies constantly, try reducing the `BATCH_SIZE` and `MAX_SEQ_LEN` in `MTDNNConfig`, but note that model performance may be compromised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uninstall the default 1.4.0 and run the following command (due to the config of our dev box)\n",
    "# !pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Classification of MultiNLI Sentences using MT-DNN\n",
    "\n",
    "This notebook utilizes the pip installable package that implements the Multi-Task Deep Neural Network Toolkit (MTDNN) for Natural Language Understanding. It's recommended to run this notebook on GPU machines as it's very computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from mtdnn.common.types import EncoderModelType\n",
    "from mtdnn.configuration_mtdnn import MTDNNConfig\n",
    "from mtdnn.data_builder_mtdnn import MTDNNDataBuilder\n",
    "from mtdnn.modeling_mtdnn import MTDNNModel\n",
    "from mtdnn.process_mtdnn import MTDNNDataProcess\n",
    "from mtdnn.tasks.config import MTDNNTaskDefs\n",
    "from mtdnn.tokenizer_mtdnn import MTDNNTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Configuration, Tasks and Model Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configuration, Tasks and Model Objects\n",
    "ROOT_DIR = TemporaryDirectory().name\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'checkpoint')\n",
    "os.makedirs(OUTPUT_DIR) if not os.path.exists(OUTPUT_DIR) else OUTPUT_DIR\n",
    "\n",
    "LOG_DIR = os.path.join(ROOT_DIR, 'tensorboard_logdir')\n",
    "os.makedirs(LOG_DIR) if not os.path.exists(LOG_DIR) else LOG_DIR\n",
    "\n",
    "DATA_DIR = \"../../../glue_data/\"\n",
    "TASK_DATA_DIRS = {\n",
    "    'qqp': os.path.join(DATA_DIR, \"QQP\"),\n",
    "    'mnli': os.path.join(DATA_DIR, \"MNLI\"),\n",
    "    'sst': os.path.join(DATA_DIR, \"SST-2\"),\n",
    "    'mrpc': os.path.join(DATA_DIR, \"MRPC\")\n",
    "    }\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16\n",
    "MULTI_GPU_ON = False\n",
    "MAX_SEQ_LEN = 128\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qqp': '../../../glue_data/QQP',\n",
       " 'mnli': '../../../glue_data/MNLI',\n",
       " 'sst': '../../../glue_data/SST-2',\n",
       " 'mrpc': '../../../glue_data/MRPC'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK_DATA_DIRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the location for our data to be downloaded, model to be checkpointed and logs to be dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpkl3fv4wv/checkpoint\n",
      "/tmp/tmpkl3fv4wv/tensorboard_logdir\n"
     ]
    }
   ],
   "source": [
    "print(OUTPUT_DIR)\n",
    "print(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Configuration Object \n",
    "\n",
    "Create a model configuration object, `MTDNNConfig`, with the necessary parameters to initialize the MT-DNN model. Initialization without any parameters will default to a similar configuration that initializes a BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MTDNNConfig(batch_size=BATCH_SIZE, \n",
    "                     max_seq_len=MAX_SEQ_LEN, \n",
    "                     multi_gpu_on=MULTI_GPU_ON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Task Definition Object  \n",
    "\n",
    "Define the task parameters to train for and initialize an `MTDNNTaskDefs` object. Create a task parameter dictionary. Definition can be a single or multiple tasks to train.  `MTDNNTaskDefs` can take a python dict, yaml or json file with task(s) defintion.\n",
    "\n",
    "The data source directory is the path of data downloaded and extracted above using `download_tsv_files_and_extract` which is the `MNLI` dir under the `DATA_DIR` temporary directory.    \n",
    "\n",
    "The data source has options that are set to drive each task pre-processing; `data_process_opts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:21 - mtdnn.tasks.config - INFO - Mapping Task attributes\n",
      "11/02/2020 11:53:21 - mtdnn.tasks.config - INFO - Mapping Task attributes\n",
      "11/02/2020 11:53:21 - mtdnn.tasks.config - INFO - Configured task definitions - ['mrpc', 'sst']\n"
     ]
    }
   ],
   "source": [
    "default_data_process_opts = {\"header\": True, \"is_train\": True, \"multi_snli\": False,}\n",
    "default_split_names = [\"train\", \"dev\", \"test\"]\n",
    "tasks_params = {\n",
    "#     \"mnli\": {\n",
    "#         \"data_format\": \"PremiseAndOneHypothesis\",\n",
    "#         \"encoder_type\": \"BERT\",\n",
    "#         \"dropout_p\": 0.3,\n",
    "#         \"enable_san\": True,\n",
    "#         \"labels\": [\"contradiction\", \"neutral\", \"entailment\"],\n",
    "#         \"metric_meta\": [\"ACC\"],\n",
    "#         \"loss\": \"CeCriterion\",\n",
    "#         \"kd_loss\": \"MseCriterion\",\n",
    "#         \"n_class\": 3,\n",
    "#         \"split_names\": [\n",
    "#             \"train\",\n",
    "#             \"dev_matched\",\n",
    "#             \"dev_mismatched\",\n",
    "#             \"test_matched\",\n",
    "#             \"test_mismatched\",\n",
    "#         ],\n",
    "#         \"data_source_dir\": TASK_DATA_DIRS['mnli'],\n",
    "#         \"data_process_opts\": {\"header\": True, \"is_train\": True, \"multi_snli\": False,},\n",
    "#         \"task_type\": \"Classification\",\n",
    "#     },\n",
    "    \"mrpc\": {\n",
    "                \"task_name\": \"mrpc\",\n",
    "                \"data_format\": \"PremiseAndOneHypothesis\",\n",
    "                \"encoder_type\": \"BERT\",\n",
    "                \"enable_san\": True,\n",
    "                \"metric_meta\": [\"ACC\", \"F1\"],\n",
    "                \"loss\": \"CeCriterion\",\n",
    "                \"kd_loss\": \"MseCriterion\",\n",
    "                \"n_class\": 2,\n",
    "                \"split_names\": default_split_names,\n",
    "                \"data_source_dir\": TASK_DATA_DIRS['mrpc'],\n",
    "                \"data_process_opts\": default_data_process_opts,\n",
    "                \"task_type\": \"Classification\",\n",
    "            },\n",
    "    \"sst\": {\n",
    "                \"task_name\": \"sst\",\n",
    "                \"data_format\": \"PremiseOnly\",\n",
    "                \"encoder_type\": \"BERT\",\n",
    "                \"enable_san\": False,\n",
    "                \"metric_meta\": [\"ACC\"],\n",
    "                \"loss\": \"CeCriterion\",\n",
    "                \"kd_loss\": \"MseCriterion\",\n",
    "                \"n_class\": 2,\n",
    "                \"split_names\": default_split_names,\n",
    "                \"data_source_dir\": TASK_DATA_DIRS['sst'],\n",
    "                \"data_process_opts\": default_data_process_opts,\n",
    "                \"task_type\": \"Classification\",\n",
    "            },\n",
    "}\n",
    "\n",
    "# Define the tasks\n",
    "task_defs = MTDNNTaskDefs(tasks_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create the MTDNN Data Tokenizer Object  \n",
    "\n",
    "Create a data tokenizing object, `MTDNNTokenizer`. Based on the model initial checkpoint, it wraps around the model's Huggingface transformers library to encode the data to MT-DNN format. This becomes the input to the data building stage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MTDNNTokenizer(do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the Tokenizer encode function on a sample text\n",
    "`tokenizer.encode(\"What NLP toolkit do you recommend\", \"MT-DNN is a fantastic toolkit\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2054, 17953, 2361, 6994, 23615, 2079, 2017, 16755, 102, 102],\n",
       " None,\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single sentence\n",
    "tokenizer.encode(\"What NLP toolkit do you recommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 2054, 17953, 2361, 6994, 23615, 2079, 2017, 16755, 102, 11047, 1011, 1040, 10695, 2003, 1037, 10392, 6994, 23615, 102], None, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# sentence pair\n",
    "print(tokenizer.encode(\"What NLP toolkit do you recommend\", \"MT-DNN is a fantastic toolkit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Builder Object  \n",
    "\n",
    "Create a data preprocessing object, `MTDNNDataBuilder`. This class is responsible for converting the data into the MT-DNN format depending on the task.  \n",
    " \n",
    "\n",
    "Define a data builder that handles the creating of each task's vectorized data utilizing the model tokenizer. This will build out the vectorized data needed for creating the training, test and development PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrpc': {'data_paths': ['../../../glue_data/MRPC/train.tsv',\n",
       "   '../../../glue_data/MRPC/dev.tsv',\n",
       "   '../../../glue_data/MRPC/test.tsv'],\n",
       "  'data_opts': {'header': True, 'is_train': True, 'multi_snli': False}},\n",
       " 'sst': {'data_paths': ['../../../glue_data/SST-2/train.tsv',\n",
       "   '../../../glue_data/SST-2/dev.tsv',\n",
       "   '../../../glue_data/SST-2/test.tsv'],\n",
       "  'data_opts': {'header': True, 'is_train': True, 'multi_snli': False}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_defs.data_paths_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 3668 samples for mrpc at ./canonical_data/mrpc_train.tsv\n",
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 408 samples for mrpc at ./canonical_data/mrpc_dev.tsv\n",
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 1725 samples for mrpc at ./canonical_data/mrpc_test.tsv\n",
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 8000 samples for sst at ./canonical_data/sst_train.tsv\n",
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 2000 samples for sst at ./canonical_data/sst_dev.tsv\n",
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Sucessfully loaded and built 1821 samples for sst at ./canonical_data/sst_test.tsv\n",
      "mrpc_train\n",
      "11/02/2020 11:53:30 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MRPC TRAIN' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Data For Premise and One Hypothesis: 3668it [00:03, 1034.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:34 - mtdnn.data_builder_mtdnn - INFO - Saving data to ./canonical_data/bert_base_uncased/mrpc_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 3668/3668 [00:00<00:00, 33142.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrpc_dev\n",
      "11/02/2020 11:53:34 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MRPC DEV' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 408it [00:00, 1013.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:34 - mtdnn.data_builder_mtdnn - INFO - Saving data to ./canonical_data/bert_base_uncased/mrpc_dev.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 408/408 [00:00<00:00, 28435.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrpc_test\n",
      "11/02/2020 11:53:34 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'MRPC TEST' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise and One Hypothesis: 1725it [00:01, 1016.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:36 - mtdnn.data_builder_mtdnn - INFO - Saving data to ./canonical_data/bert_base_uncased/mrpc_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseAndOneHypothesis: 100%|██████████| 1725/1725 [00:00<00:00, 31342.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst_train\n",
      "11/02/2020 11:53:36 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'SST TRAIN' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise Only: 8000it [00:02, 3489.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:38 - mtdnn.data_builder_mtdnn - INFO - Saving data to ./canonical_data/bert_base_uncased/sst_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseOnly: 100%|██████████| 8000/8000 [00:00<00:00, 76292.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst_dev\n",
      "11/02/2020 11:53:38 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'SST DEV' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise Only: 2000it [00:00, 2648.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:39 - mtdnn.data_builder_mtdnn - INFO - Saving data to ./canonical_data/bert_base_uncased/sst_dev.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseOnly: 100%|██████████| 2000/2000 [00:00<00:00, 61990.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst_test\n",
      "11/02/2020 11:53:39 - mtdnn.data_builder_mtdnn - INFO - Building Data For 'SST TEST' Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Data For Premise Only: 1821it [00:01, 1782.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:40 - mtdnn.data_builder_mtdnn - INFO - Saving data to ./canonical_data/bert_base_uncased/sst_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Data For PremiseOnly: 100%|██████████| 1821/1821 [00:00<00:00, 46184.06it/s]\n"
     ]
    }
   ],
   "source": [
    "## Load and build data\n",
    "data_builder = MTDNNDataBuilder(\n",
    "    tokenizer=tokenizer,\n",
    "    task_defs=task_defs,\n",
    "    data_dir='.', #DATA_SOURCE_DIR,\n",
    "    canonical_data_suffix=\"canonical_data\",\n",
    "    dump_rows=True,\n",
    ")\n",
    "\n",
    "## Build data to MTDNN Format\n",
    "## Iterable of each specific task and processed data\n",
    "vectorized_data = data_builder.vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Processing Object  \n",
    "\n",
    "Create a data preprocessing object, `MTDNNDataProcess`. This creates the training, test and development PyTorch dataloaders needed for training and testing. We also need to retrieve the necessary training options required to initialize the model correctly, for all tasks.  \n",
    "\n",
    "Define a data process that handles creating the training, test and development PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Starting to process the training data sets\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Loading mrpc_train as task 0\n",
      "11/02/2020 11:53:48 - mtdnn.dataset_mtdnn - INFO - Loaded 3668 samples out of 3668\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Loading sst_train as task 1\n",
      "11/02/2020 11:53:48 - mtdnn.dataset_mtdnn - INFO - Loaded 8000 samples out of 8000\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Starting to process the testing data sets\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Loading mrpc_dev as task 0\n",
      "11/02/2020 11:53:48 - mtdnn.dataset_mtdnn - INFO - Loaded 408 samples out of 408\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Loading mrpc_test as task 0\n",
      "11/02/2020 11:53:48 - mtdnn.dataset_mtdnn - INFO - Loaded 1725 samples out of 1725\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Loading sst_dev as task 1\n",
      "11/02/2020 11:53:48 - mtdnn.dataset_mtdnn - INFO - Loaded 2000 samples out of 2000\n",
      "11/02/2020 11:53:48 - mtdnn.process_mtdnn - INFO - Loading sst_test as task 1\n",
      "11/02/2020 11:53:48 - mtdnn.dataset_mtdnn - INFO - Loaded 1821 samples out of 1821\n"
     ]
    }
   ],
   "source": [
    "# Make the Data Preprocess step and update the config with training data updates\n",
    "data_processor = MTDNNDataProcess(\n",
    "    config=config, task_defs=task_defs, vectorized_data=vectorized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the processed batch multitask batch data loaders for training, development and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_train_dataloader = data_processor.get_train_dataloader()\n",
    "dev_dataloaders_list = data_processor.get_dev_dataloaders()\n",
    "test_dataloaders_list = data_processor.get_test_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve the training options, from the processor, to initialize model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_opts = data_processor.get_decoder_options_list()\n",
    "task_types = data_processor.get_task_types_list()\n",
    "dropout_list = data_processor.get_tasks_dropout_prob_list()\n",
    "loss_types = data_processor.get_loss_types_list()\n",
    "kd_loss_types = data_processor.get_kd_loss_types_list()\n",
    "tasks_nclass_list = data_processor.get_task_nclass_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us update the batch steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3650"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_all_batches = data_processor.get_num_all_batches()\n",
    "num_all_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the MTDNN Model\n",
    "\n",
    "Now we can go ahead and create an `MTDNNModel` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, number of task labels: 2\n",
      "idx: 1, number of task labels: 2\n"
     ]
    }
   ],
   "source": [
    "model = MTDNNModel(\n",
    "    config,\n",
    "    task_defs,\n",
    "    pretrained_model_name=\"bert-base-uncased\",\n",
    "    num_train_step=num_all_batches,\n",
    "    decoder_opts=decoder_opts,\n",
    "    task_types=task_types,\n",
    "    dropout_list=dropout_list,\n",
    "    loss_types=loss_types,\n",
    "    kd_loss_types=kd_loss_types,\n",
    "    tasks_nclass_list=tasks_nclass_list,\n",
    "    multitask_train_dataloader=multitask_train_dataloader,\n",
    "    dev_dataloaders_list=dev_dataloaders_list,\n",
    "    test_dataloaders_list=test_dataloaders_list,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    log_dir=LOG_DIR \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Finetuning, Prediction and Evaluation\n",
    "\n",
    "### Fit and finetune model on five epochs and predict using the training and test  \n",
    "\n",
    "At this point the MT-DNN model allows us to fit to the model and create predictions. The fit takes an optional `epochs` parameter that overwrites the epochs set in the `MTDNNConfig` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - Total number of params: 109485316\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - Total number of params: 109485316\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - At epoch 0\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - At epoch 0\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [     1] Training Loss - [0.66444] Time Remaining - [0:02:37]\n",
      "11/02/2020 11:28:51 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [     1] Training Loss - [0.66444] Time Remaining - [0:02:37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feifang/mtl-epistemic-uncertainty/microsoft-mt-dnn/mtdnn/common/bert_optim.py:144: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:30:44 - mtdnn.modeling_mtdnn - INFO - Task - [ 0] Updates - [   500] Training Loss - [0.51931] Time Remaining - [0:00:51]\n",
      "11/02/2020 11:30:44 - mtdnn.modeling_mtdnn - INFO - Task - [ 0] Updates - [   500] Training Loss - [0.51931] Time Remaining - [0:00:51]\n",
      "11/02/2020 11:31:37 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_0.pt\n",
      "11/02/2020 11:31:37 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_0.pt\n",
      "11/02/2020 11:31:43 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_0.pt\n",
      "11/02/2020 11:31:43 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_0.pt\n",
      "11/02/2020 11:31:43 - mtdnn.modeling_mtdnn - INFO - At epoch 1\n",
      "11/02/2020 11:31:43 - mtdnn.modeling_mtdnn - INFO - At epoch 1\n",
      "11/02/2020 11:31:43 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:31:43 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:32:45 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  1000] Training Loss - [0.43466] Time Remaining - [0:01:45]\n",
      "11/02/2020 11:32:45 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  1000] Training Loss - [0.43466] Time Remaining - [0:01:45]\n",
      "11/02/2020 11:34:30 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_1.pt\n",
      "11/02/2020 11:34:30 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_1.pt\n",
      "11/02/2020 11:34:33 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_1.pt\n",
      "11/02/2020 11:34:33 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_1.pt\n",
      "11/02/2020 11:34:33 - mtdnn.modeling_mtdnn - INFO - At epoch 2\n",
      "11/02/2020 11:34:33 - mtdnn.modeling_mtdnn - INFO - At epoch 2\n",
      "11/02/2020 11:34:33 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:34:33 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:34:43 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  1500] Training Loss - [0.37944] Time Remaining - [0:02:40]\n",
      "11/02/2020 11:34:43 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  1500] Training Loss - [0.37944] Time Remaining - [0:02:40]\n",
      "11/02/2020 11:36:37 - mtdnn.modeling_mtdnn - INFO - Task - [ 0] Updates - [  2000] Training Loss - [0.33606] Time Remaining - [0:00:43]\n",
      "11/02/2020 11:36:37 - mtdnn.modeling_mtdnn - INFO - Task - [ 0] Updates - [  2000] Training Loss - [0.33606] Time Remaining - [0:00:43]\n",
      "11/02/2020 11:37:20 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_2.pt\n",
      "11/02/2020 11:37:20 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_2.pt\n",
      "11/02/2020 11:37:25 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_2.pt\n",
      "11/02/2020 11:37:25 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_2.pt\n",
      "11/02/2020 11:37:25 - mtdnn.modeling_mtdnn - INFO - At epoch 3\n",
      "11/02/2020 11:37:25 - mtdnn.modeling_mtdnn - INFO - At epoch 3\n",
      "11/02/2020 11:37:25 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:37:25 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:38:36 - mtdnn.modeling_mtdnn - INFO - Task - [ 0] Updates - [  2500] Training Loss - [0.30319] Time Remaining - [0:01:36]\n",
      "11/02/2020 11:38:36 - mtdnn.modeling_mtdnn - INFO - Task - [ 0] Updates - [  2500] Training Loss - [0.30319] Time Remaining - [0:01:36]\n",
      "11/02/2020 11:40:12 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_3.pt\n",
      "11/02/2020 11:40:12 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_3.pt\n",
      "11/02/2020 11:40:15 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_3.pt\n",
      "11/02/2020 11:40:15 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_3.pt\n",
      "11/02/2020 11:40:15 - mtdnn.modeling_mtdnn - INFO - At epoch 4\n",
      "11/02/2020 11:40:15 - mtdnn.modeling_mtdnn - INFO - At epoch 4\n",
      "11/02/2020 11:40:15 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:40:15 - mtdnn.modeling_mtdnn - INFO - Amount of data to go over: 730\n",
      "11/02/2020 11:40:34 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  3000] Training Loss - [0.27759] Time Remaining - [0:02:31]\n",
      "11/02/2020 11:40:34 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  3000] Training Loss - [0.27759] Time Remaining - [0:02:31]\n",
      "11/02/2020 11:42:27 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  3500] Training Loss - [0.25311] Time Remaining - [0:00:34]\n",
      "11/02/2020 11:42:27 - mtdnn.modeling_mtdnn - INFO - Task - [ 1] Updates - [  3500] Training Loss - [0.25311] Time Remaining - [0:00:34]\n",
      "11/02/2020 11:43:01 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n",
      "11/02/2020 11:43:01 - mtdnn.modeling_mtdnn - INFO - Saving mt-dnn model to /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n",
      "11/02/2020 11:43:04 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n",
      "11/02/2020 11:43:04 - mtdnn.modeling_mtdnn - INFO - model saved to /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n"
     ]
    }
   ],
   "source": [
    "model.fit(epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction\n",
    "Perform inference using the last (best) checkpointed model. With 5 epochs, the last model would be `model_4.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/02/2020 11:54:02 - mtdnn.modeling_mtdnn - INFO - Running predictions using: /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n",
      "11/02/2020 11:54:02 - mtdnn.modeling_mtdnn - INFO - Running predictions using: /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n",
      "11/02/2020 11:54:02 - mtdnn.modeling_mtdnn - INFO - Running predictions using: /tmp/tmpkl3fv4wv/checkpoint/model_4.pt\n",
      "11/02/2020 11:54:03 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:03 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:03 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:05 - mtdnn.modeling_mtdnn - INFO - Task sst -- epoch 0 -- Dev ACC: 86.520\n",
      "11/02/2020 11:54:05 - mtdnn.modeling_mtdnn - INFO - Task sst -- epoch 0 -- Dev ACC: 86.520\n",
      "11/02/2020 11:54:05 - mtdnn.modeling_mtdnn - INFO - Task sst -- epoch 0 -- Dev ACC: 86.520\n",
      "11/02/2020 11:54:05 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:05 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:05 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:09 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:09 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:09 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:12 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:12 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:12 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:13 - mtdnn.modeling_mtdnn - INFO - [new test scores saved.]\n",
      "11/02/2020 11:54:13 - mtdnn.modeling_mtdnn - INFO - [new test scores saved.]\n",
      "11/02/2020 11:54:13 - mtdnn.modeling_mtdnn - INFO - [new test scores saved.]\n",
      "11/02/2020 11:54:13 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:13 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:13 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:15 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:15 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:15 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:17 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:17 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:17 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - Task mrpc -- epoch 0 -- Dev ACC: 90.800\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - Task mrpc -- epoch 0 -- Dev ACC: 90.800\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - Task mrpc -- epoch 0 -- Dev ACC: 90.800\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - Task mrpc -- epoch 0 -- Dev F1: 91.606\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - Task mrpc -- epoch 0 -- Dev F1: 91.606\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - Task mrpc -- epoch 0 -- Dev F1: 91.606\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:18 - mtdnn.modeling_mtdnn - INFO - predicting 0\n",
      "11/02/2020 11:54:20 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:20 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:20 - mtdnn.modeling_mtdnn - INFO - predicting 100\n",
      "11/02/2020 11:54:23 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:23 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:23 - mtdnn.modeling_mtdnn - INFO - predicting 200\n",
      "11/02/2020 11:54:23 - mtdnn.modeling_mtdnn - INFO - [new test scores saved.]\n",
      "11/02/2020 11:54:23 - mtdnn.modeling_mtdnn - INFO - [new test scores saved.]\n",
      "11/02/2020 11:54:23 - mtdnn.modeling_mtdnn - INFO - [new test scores saved.]\n"
     ]
    }
   ],
   "source": [
    "model.predict(trained_model_chckpt=f\"{OUTPUT_DIR}/model_4.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mrpc Dev Scores</th>\n",
       "      <th>Sst Dev Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACCURACY</th>\n",
       "      <td>90.800</td>\n",
       "      <td>86.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Mrpc Dev Scores Sst Dev Scores\n",
       "ACCURACY          90.800         86.520"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "dev_result_files = list(filter(lambda x: x.endswith('.json') and 'dev' in x, os.listdir(OUTPUT_DIR))) \n",
    "for d in dev_result_files: \n",
    "    name =  ' '.join(list(map(str.capitalize, d.split('_')))[:3]) \n",
    "    file_name = os.path.join(OUTPUT_DIR, d)\n",
    "    with open(file_name, 'r') as f: \n",
    "        res = json.load(f) \n",
    "        results.update(\n",
    "            {name: {\n",
    "                'ACCURACY': f\"{res['metrics']['ACC']:.3f}\"\n",
    "                }\n",
    "            }) \n",
    "df_results = pd.DataFrame(results)   \n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up temporary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ROOT_DIR):\n",
    "    shutil.rmtree(ROOT_DIR, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
